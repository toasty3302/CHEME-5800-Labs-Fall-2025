{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d205e79",
   "metadata": {},
   "source": [
    "# L6d: Another Look at Solving Linear Equations\n",
    "In this activity, we will revisit the solution of linear algebraic equations, where we will compare the solutions we obtain by using QR decomposition with our iterative methods for solving linear equations.\n",
    "\n",
    "> **Learning objectives:**\n",
    ">\n",
    "> In this lab, students will learn to:\n",
    ">\n",
    "> * **Compare direct and iterative solution methods.** We solve a manufactured 1D Poisson equation using both QR decomposition and iterative methods (Jacobi, Gauss-Seidel, SOR) to understand their accuracy and convergence behavior.\n",
    "> * **Analyze computational performance trade-offs.** We benchmark memory usage, execution time, and solution accuracy across different linear algebra approaches to determine optimal method selection.\n",
    "> * **Validate numerical solutions using manufactured solutions.** We construct test problems with known exact solutions to measure and compare the precision of different linear equation solvers.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68491b3f",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> __Include:__ The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on Julia functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964d1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include-solution.jl\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e365ab9d",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types and data used in this material. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb77e3d",
   "metadata": {},
   "source": [
    "## Task 1: Set up the System of Linear Equations\n",
    "In this task, we will set up a system of linear equations that we will solve using both QR decomposition and iterative methods. We will define the coefficient matrix $\\mathbf{A}$ and the right-hand side vector $\\mathbf{b}$ for the system of equations $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$.\n",
    "\n",
    "> __Test Problem (method of manufactured solutions)__\n",
    "> \n",
    "> Let's solve a classic problem: the **one-dimensional Poisson equation** with **Dirichlet boundary conditions**. This shows up everywhere in physics and engineering (heat conduction, electrostatics, fluid flow, you name it). However, instead of solving it directly, we'll use a trick called the **method of manufactured solutions** to create a test problem with a known solution. \n",
    "> \n",
    "> * __Idea:__ We pick $u(x) = \\sin(\\pi x)$ as our known answer, discretize the domain into $n$ grid points at $x_i = \\frac{i}{n+1}$, and use finite differences to approximate derivatives. The continuous equation $-u''(x) = f(x)$ becomes the discrete system $-\\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2} = f_i$.\n",
    "> * __Manufactured solution approach__: In the manufactured solution approach, we don't actually need to know what $f(x)$ is! Since we already decided that $u(x) = \\sin(\\pi x)$ is our solution, we can just compute $\\mathbf{b} = \\mathbf{A} \\mathbf{x}_{true}$ directly. The vector $\\mathbf{b}$ represents the discrete version of $f(x)$ that would produce our chosen solution. It's like saying \"if this is the answer, what must the question have been?\"\n",
    ">\n",
    "> This approach gives us a **tridiagonal system** $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ where $\\mathbf{A}$ has 2's on the diagonal and -1's on the off-diagonals. Since we know the exact answer, we can measure how accurate our different numerical methods actually are!\n",
    "\n",
    "Let's set up our system of equations. We save the system matrix in the `A::Array{Float64,2}` variable, the right-hand side vector in the `b::Array{Float64,1}` variable, and the true solution in the `x_true::Array{Float64,1}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ad2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "A,b,x_true = let \n",
    "    \n",
    "    n = 100; # how big of a system do we want to solve?\n",
    "\n",
    "    # Build the tridiagonal matrix A and the manufactured solution x⋆ and right-hand side b\n",
    "    # for the 1D Poisson equation with Dirichlet boundary conditions.\n",
    "    # The matrix A is constructed using the sparse diagonal matrix constructor `spdiagm`.\n",
    "    di = fill(2.0, n); dl = fill(-1.0, n-1); du = fill(-1.0, n-1)\n",
    "    A  = diagm(-1 => dl, 0 => di, 1 => du)              # tridiagonal matrix A  \n",
    "    xg = range(1, n; step=1) ./ (n+1)                   # grid i/(n+1)\n",
    "    xstar = @. sin(pi * xg)                             # \"true\" solution\n",
    "    b = A * xstar                                       # manufactured RHS\n",
    "\n",
    "    A,b, xstar # return\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781116b8",
   "metadata": {},
   "source": [
    "What does the system matrix $\\mathbf{A}$ look like for this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "458009e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×100 Matrix{Float64}:\n",
       "  2.0  -1.0   0.0   0.0   0.0   0.0  …   0.0   0.0   0.0   0.0   0.0   0.0\n",
       " -1.0   2.0  -1.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0  -1.0   2.0  -1.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0  -1.0   2.0  -1.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0  -1.0   2.0  -1.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0  -1.0   2.0  …   0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0  -1.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0  …   0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  ⋮                             ⋮    ⋱         ⋮                      \n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0  …   0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0     -1.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      2.0  -1.0   0.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0  …  -1.0   2.0  -1.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      0.0  -1.0   2.0  -1.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0  -1.0   2.0  -1.0   0.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0  -1.0   2.0  -1.0\n",
       "  0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0  -1.0   2.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807cad0",
   "metadata": {},
   "source": [
    "### Check: Is the system matrix $\\mathbf{A}$ diagonally dominant?\n",
    "Before we continue, let's verify the generated system matrix $\\mathbf{A}$ is actually diagonally dominant.\n",
    "> __Test:__ We'll compute the sum of the absolute values of each row (excluding the diagonal element), and compare it to the absolute value of the diagonal element in that row. If the sum of the absolute values of the non-diagonal elements is less than the absolute value of the diagonal element, then the matrix is strongly diagonally dominant. If the sum is equal to the diagonal element, then the matrix is weakly diagonally dominant.\n",
    "\n",
    "Is the matrix $\\mathbf{A}$ (weakly) diagonally dominant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ee41047",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddcondition = let\n",
    "    \n",
    "    # initialize -\n",
    "    number_of_rows = size(A, 1);\n",
    "    ddcondition = Array{Bool,1}(undef, number_of_rows);\n",
    "\n",
    "    # let's check each row\n",
    "    for i ∈ 1:number_of_rows\n",
    "        aii = abs(A[i,i]);\n",
    "        σ = 0.0;\n",
    "        for j ∈ 1:number_of_rows\n",
    "            if (i ≠ j)\n",
    "                σ += abs(A[i,j]);\n",
    "            end\n",
    "        end\n",
    "        ddcondition[i] = (aii ≥ σ) ? true : false; # ternary operator, nice! # TODO: Notice we use ≥ here, not >, so we allow for weak diagonal dominance\n",
    "    end\n",
    "\n",
    "    ddcondition\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7cd491",
   "metadata": {},
   "source": [
    "The `ddcondition` array contains boolean values indicating whether each row of our system matrix $\\mathbf{A}$ satisfies the diagonal dominance condition. Each element `ddcondition[i]` is `true` if the absolute value of the diagonal element in row `i` is greater than the sum of the absolute values of all other elements in that row, and `false` otherwise.\n",
    "\n",
    "> __Test:__ The assertion `@assert any(ddcondition)` checks that at least one row (and hopefully all rows) satisfies the diagonal dominance condition. If the condition fails, a warning is issued indicating that the matrix may not be suitable for iterative methods, as convergence is not guaranteed.\n",
    "\n",
    "For diagonally dominant matrices, iterative methods like Jacobi, Gauss-Seidel, and SOR are guaranteed to converge, making them reliable choices for solving linear systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68179802",
   "metadata": {},
   "outputs": [],
   "source": [
    "try\n",
    "    @assert any(ddcondition .== false) == false; # if any are false, then the assertion fails\n",
    "catch\n",
    "    @warn \"Diagonal dominance condition not satisfied\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40db56",
   "metadata": {},
   "source": [
    "### Check: Is our test solution correct for the system of equations?\n",
    "We generated a manufactured solution vector `x_true::Array{Float64,1}` and computed the corresponding right-hand side vector `b::Array{Float64,1}` using the system matrix `A::Array{Float64,2}`. Now, we will verify that our generated solution is indeed correct by checking if the equation $\\mathbf{A} \\mathbf{x}_{\\text{true}} = \\mathbf{b}$ holds true.\n",
    "\n",
    "> __Test:__ We will compute the product of the system matrix `A` and the solution vector `x_true`, and compare it to the right-hand side vector `b`. If they are (approximately) equal, then our generated solution is correct. We will use [the `isapprox(...)` function](https://docs.julialang.org/en/v1/base/math/#Base.isapprox) to check for approximate equality in combination with [the `@assert` macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert) to ensure that the test passes.\n",
    "\n",
    "So what do we see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0e94569",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    atol = 1e-10; # absolute tolerance for checking the residual norm against zero\n",
    "\n",
    "    # compute the residual for the manufactured solution\n",
    "    r = b - A*x_true # if r ≈ 0, then x_true is a solution to the system of equations Ax = b\n",
    "    norm_r = norm(r); # 2-norm of the residual vector\n",
    "    \n",
    "    # run the test -\n",
    "    @assert isapprox(norm_r, 0.0; atol = atol);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8048544d",
   "metadata": {},
   "source": [
    "If we get through without any warnings or assertion errors, then our generated solution is indeed correct for the system of equations defined by the matrix `A` and the vector `b`!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531211df",
   "metadata": {},
   "source": [
    "## Task 2: Compare the QR decomposition solution with the true solution\n",
    "In this task, we will solve the system of linear equations using QR decomposition and compare the solution we obtain with the true solution vector `x_true` that we generated earlier.\n",
    "\n",
    "> __QR Decomposition__\n",
    ">\n",
    "> The QR decomposition can be used as a direct method for solving linear systems. The QR decomposition factors a matrix into two matrices, $\\mathbf{Q}$ and $\\mathbf{R}$, such that $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$. The matrix $\\mathbf{Q}$ is orthogonal, meaning that its columns are orthonormal vectors, which gives us the property that $\\mathbf{Q}^{\\top} \\mathbf{Q} = \\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix. The matrix $\\mathbf{R}$ is an upper triangular matrix.\n",
    ">\n",
    "> This is super handy for solving linear systems of equations:\n",
    ">$$\n",
    "\\begin{align*}\n",
    "\\mathbf{A} \\mathbf{x} &= \\mathbf{b}\\quad\\Longrightarrow{\\text{decompose}}\\;\\mathbf{A} = \\mathbf{Q} \\mathbf{R} \\\\\n",
    "\\left(\\mathbf{Q} \\mathbf{R}\\right) \\mathbf{x} &= \\mathbf{b}\\quad\\Longrightarrow\\text{multiply by } \\mathbf{Q}^{\\top} \\\\\n",
    "\\underbrace{\\left(\\mathbf{Q}^{\\top} \\mathbf{Q}\\right)}_{=\\mathbf{I}} \\mathbf{R} \\mathbf{x} &= \\mathbf{Q}^{\\top} \\mathbf{b}\\quad\\Longrightarrow\\text{solve for } \\mathbf{x} \\\\\n",
    "\\mathbf{x} &= \\mathbf{R}^{-1} \\mathbf{Q}^{\\top} \\mathbf{b}\\quad\\blacksquare\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "QR decomposition is particularly useful for solving linear systems because it avoids the need to compute the inverse of the matrix $\\mathbf{A}$ directly, which can be computationally expensive and numerically unstable. Let's compute the QR decomposition of the matrix `A`, solve for the solution vector `x_qr::Array{Float64,1}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58331eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_qr = let\n",
    "\n",
    "    Q,R = qr(A);\n",
    "    x_qr = R \\ (transpose(Q) * b); # TODO: Note that we use the backslash operator here to solve the triangular system Rx = Qᵀb\n",
    "\n",
    "    x_qr # return the solution\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed149163",
   "metadata": {},
   "source": [
    "Next, compare `x_qr::Array{Float64,1}` and `x_true::Array{Float64,1}` using [the `isapprox(...)` function](https://docs.julialang.org/en/v1/base/math/#Base.isapprox) to check for approximate equality.\n",
    "\n",
    "> __Test:__ We will use the `isapprox(...)` function to check if the 2-norm of the difference between the two solution vectors is within a specified tolerance. If they are approximately equal, then our QR decomposition solution is correct. If not, an assertion error will be raised.\n",
    "\n",
    "So what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21b316a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    atol = 1e-10; # absolute tolerance for checking the residual norm against zero\n",
    "\n",
    "    # compute the norm of the difference between the computed solution and the manufactured solution\n",
    "    r = x_qr - x_true; # if r ≈ 0, then x_qr is approximately equal to x_true\n",
    "    norm_r = norm(r); # 2-norm of the residual vector\n",
    "    \n",
    "    # run the test -\n",
    "    @assert isapprox(norm_r, 0.0; atol = atol); # test\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd394d",
   "metadata": {},
   "source": [
    "__Ok!__ If we didn't blow up, we are good. QR decomposition gives us a solution that is approximately equal to the true solution we generated earlier. This confirms that our QR decomposition approach is valid for solving the system of linear equations defined by the matrix `A` and the vector `b`.\n",
    "\n",
    "How about our iterative methods? Do they also give us a solution that is approximately equal to the true solution `x_true`? Let's check that next!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3297eb87",
   "metadata": {},
   "source": [
    "## Task 3: Compare the iterative method solutions with the true solution\n",
    "In this task, we will solve the system of linear equations using the iterative methods we have implemented (Jacobi, Gauss-Seidel, and SOR) and compare the solutions we obtain with the true solution vector `x_true` that we generated earlier.\n",
    "\n",
    "First, let's define the initial guess for the iterative methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6fb4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows = size(A, 1);\n",
    "xₒ = 0.1*ones(number_of_rows); # initial solution guess xₒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c1b8f",
   "metadata": {},
   "source": [
    "### Curious: Compute the theoretical convergence bound on steps.\n",
    "Since we have the true solution `x_true`, we can compute the theoretical convergence bound on the number of iterations required for the iterative methods to converge to a solution that is approximately equal to the true solution.\n",
    "\n",
    "To reach a desired accuracy $\\epsilon$ at iteration $k$, we can bound the error as:\n",
    "$$\n",
    "\\|\\mathbf{e}^{(k)}\\| \\leq q^{k}\\,\\|\\mathbf{e}^{(0)}\\| \\leq \\epsilon\n",
    "$$\n",
    "which implies:\n",
    "$$\n",
    "\\boxed{\n",
    "   k\\geq\\frac{\\ln(\\epsilon/\\|\\mathbf{e}^{(0)}\\|)}{|\\ln(\\lVert\\mathbf{G}\\rVert)|}\n",
    "\\quad\\blacksquare}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{G}$ is the iteration matrix for the chosen iterative method, $\\mathbf{e}^{(0)}$ is the initial error (i.e., the difference between the initial guess and the true solution), and $q = \\lVert\\mathbf{G}\\rVert$ is some norm of the iteration matrix. Let's use the infinity norm for this example.\n",
    "\n",
    "We'll save the computed theoretical minimum number of iterations required for convergence in the `kmin::Int` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c302452f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmin = let\n",
    "\n",
    "    # initialize -\n",
    "    ϵ = 1e-8; # convergence tolerance\n",
    "    eₒ = x_true - xₒ; # initial error\n",
    "    D = diag(A) |> a-> diagm(a); # extract the diagonal of A and form a diagonal matrix\n",
    "    U = triu(A,1); # extract the upper triangular part of A, excluding the diagonal\n",
    "    L = tril(A,-1); # extract the lower triangular part of A, excluding the diagonal\n",
    "\n",
    "    # TODO: The iteration matrix will change for each method, so you will need to implement this for each method\n",
    "    # TODO: Uncomment for Jacobi method\n",
    "    # M = D;\n",
    "    # N = -(L + U);\n",
    "    # G = inv(M) * N; # iteration matrix\n",
    "\n",
    "    # TODO: Uncomment for Gauss-Seidel method\n",
    "    M = D + L;\n",
    "    N = -U;\n",
    "    G = inv(M) * N; # iteration matrix\n",
    "\n",
    "    # compute k -\n",
    "    k = abs(log(ϵ/norm(eₒ, Inf))) / abs(log(norm(G, Inf))) |> ceil |> Int; # theoretical minimum number of iterations required for convergence\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa05572",
   "metadata": {},
   "source": [
    "Next, set up a code block that allows us to compute the solution using an iterative method of our choice, and then compare the solution to the true solution vector `x_true` using the `isapprox(...)` function to check for approximate equality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "507a9399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64, Vector{Float64}} with 13753 entries:\n",
       "  11950 => [0.0310996, 0.0621691, 0.0931785, 0.124098, 0.154897, 0.185546, 0.21…\n",
       "  1703  => [0.0257491, 0.0514786, 0.0771633, 0.102779, 0.1283, 0.153701, 0.1789…\n",
       "  12427 => [0.0310997, 0.0621693, 0.0931788, 0.124098, 0.154897, 0.185547, 0.21…\n",
       "  7685  => [0.0310835, 0.0621369, 0.0931302, 0.124033, 0.154817, 0.18545, 0.215…\n",
       "  3406  => [0.0300701, 0.0601121, 0.0900969, 0.119996, 0.149779, 0.179419, 0.20…\n",
       "  1090  => [0.0214191, 0.0428268, 0.0642024, 0.0855252, 0.106774, 0.12793, 0.14…\n",
       "  2015  => [0.0271434, 0.0542644, 0.0813367, 0.108334, 0.135231, 0.162, 0.18861…\n",
       "  11280 => [0.0310994, 0.0621686, 0.0931778, 0.124097, 0.154896, 0.185545, 0.21…\n",
       "  3220  => [0.029867, 0.0597063, 0.0894891, 0.119186, 0.14877, 0.17821, 0.20747…\n",
       "  11251 => [0.0310993, 0.0621686, 0.0931777, 0.124097, 0.154896, 0.185545, 0.21…\n",
       "  422   => [0.0127757, 0.0255546, 0.0383235, 0.0510688, 0.0637773, 0.0764358, 0…\n",
       "  4030  => [0.0305369, 0.0610447, 0.0914941, 0.121855, 0.1521, 0.182197, 0.2121…\n",
       "  8060  => [0.0310885, 0.0621469, 0.0931451, 0.124053, 0.154842, 0.18548, 0.215…\n",
       "  8660  => [0.0310935, 0.0621569, 0.0931602, 0.124073, 0.154866, 0.18551, 0.215…\n",
       "  3855  => [0.030433, 0.0608372, 0.0911831, 0.121442, 0.151583, 0.181579, 0.211…\n",
       "  3163  => [0.0297971, 0.0595667, 0.0892798, 0.118908, 0.148422, 0.177794, 0.20…\n",
       "  9523  => [0.0310971, 0.0621641, 0.093171, 0.124088, 0.154884, 0.185531, 0.215…\n",
       "  2989  => [0.0295582, 0.0590893, 0.0885647, 0.117956, 0.147235, 0.176372, 0.20…\n",
       "  4357  => [0.0306896, 0.0613499, 0.0919512, 0.122464, 0.152859, 0.183106, 0.21…\n",
       "  12797 => [0.0310997, 0.0621694, 0.0931789, 0.124098, 0.154898, 0.185547, 0.21…\n",
       "  8552  => [0.0310928, 0.0621555, 0.0931581, 0.124071, 0.154863, 0.185506, 0.21…\n",
       "  9266  => [0.0310963, 0.0621625, 0.0931686, 0.124085, 0.154881, 0.185527, 0.21…\n",
       "  10454 => [0.0310987, 0.0621674, 0.0931759, 0.124094, 0.154893, 0.185541, 0.21…\n",
       "  844   => [0.0188235, 0.0376406, 0.056433, 0.0751824, 0.0938706, 0.11248, 0.13…\n",
       "  4738  => [0.0308161, 0.0616027, 0.0923299, 0.122968, 0.153488, 0.183859, 0.21…\n",
       "  ⋮     => ⋮"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_solution_archive = let\n",
    "\n",
    "    # initialize -\n",
    "    number_of_rows = size(A, 1);\n",
    "    maximum_number_of_iterations = 1000*kmin; # maximum number of iterations for the iterative method\n",
    "    tolerance = 1e-8;\n",
    "    algorithm = GaussSeidelMethod(); # change this to Jacobi(), GaussSeidelMethod() or SuccessiveOverRelaxationMethod() to test other algorithms\n",
    "    ω = 0.6; # relaxation factor, only used for SOR\n",
    "\n",
    "    # call the solve method with the appropriate parameters -\n",
    "    x = VLDataScienceMachineLearningPackage.solve(A, b, xₒ; algorithm = algorithm, \n",
    "        maxiterations = maximum_number_of_iterations, \n",
    "        ϵ = tolerance, \n",
    "        ω = ω # only used for SOR\n",
    "    );\n",
    "\n",
    "    x; # return the solution vector computed by the iterative method\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14356ea",
   "metadata": {},
   "source": [
    "### Check: Is the iterative solution actually a valid solution for the system of equations?\n",
    "In this check, we will compute the residual vector `r` for the iterative solution obtained from our iterative method and compare its norm to zero. The residual vector is defined as $\\mathbf{r}^{(k)} = \\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(k)}$, where $\\mathbf{x}^{(k)}$ is the solution obtained from the iterative method. Let's grab the last iterate from our iterative method and compute the residual vector.\n",
    "\n",
    "> __Test:__ If the norm of the residual vector is approximately zero, then the iterative solution is a valid solution for the system of equations. Let's set up a code block that computes the residual vector and checks if its norm is approximately zero using [the `isapprox(...)` function](https://docs.julialang.org/en/v1/base/math/#Base.isapprox) in combination with [the `@assert` macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert) to ensure that the test passes.\n",
    "\n",
    "So what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f75fd191",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    atol = 1e-8; # absolute tolerance\n",
    "    i = keys(iterative_solution_archive) |> collect |> sort |> last; # get the last key (the last iteration)\n",
    "    x_iterative = iterative_solution_archive[i]; # get the last solution (converged solution)\n",
    "\n",
    "    # compute the residual for the iterative solution\n",
    "    r = b - A*x_iterative; # if r ≈ 0, then x_iterative is a solution to the system of equations Ax = b\n",
    "    norm_r = norm(r); # 2-norm of the residual vector\n",
    "\n",
    "    # check: our solution vs Julia solution, should be approximately equal\n",
    "    @assert isapprox(norm_r, 0, atol=atol)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91650b4",
   "metadata": {},
   "source": [
    "So our iterative method generated a valid solution (if no warnings or assertion errors were raised), but is it approximately equal to the true solution `x_true` that we generated earlier? And, how do the choices of absolute tolerance `atol`, the maximum number of iterations and the choice of method affect the validity of the solution obtained from the iterative method?\n",
    "\n",
    "Let's check that next!\n",
    "\n",
    "> __Experiment:__ Compare the norm of the residual vector `r` between the true and iterative solutions with zero for different values of the absolute tolerance `atol`, maximum number of iterations, and different iterative methods (Jacobi, Gauss-Seidel, and SOR). How do these choices influence the validity of the solution obtained from the iterative method?\n",
    "\n",
    "What do we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0cf2e80",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "AssertionError: isapprox(norm_r, 0.0; atol = atol)",
     "output_type": "error",
     "traceback": [
      "AssertionError: isapprox(norm_r, 0.0; atol = atol)",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "\u001b[90m   @\u001b[39m \u001b[90m\u001b[4mIn[30]:13\u001b[24m\u001b[39m"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    atol = 1e-8; # absolute tolerance for checking the residual norm against zero\n",
    "    i = keys(iterative_solution_archive) |> collect |> sort |> last; # get the last key (the last iteration)\n",
    "    x_iterative = iterative_solution_archive[i]; # get the last solution (converged solution)\n",
    "\n",
    "    # compute the norm of the difference between the computed solution and the manufactured solution\n",
    "    r = x_iterative - x_true; # if r ≈ 0, then x_iterative is approximately equal to x_true\n",
    "    norm_r = norm(r); # 2-norm of the residual vector\n",
    "    \n",
    "    # run the test -\n",
    "    # @assert isapprox(norm_r, 0.0; atol = atol); # test (not working as expected??!!??)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da38ae38",
   "metadata": {},
   "source": [
    "### Curious: Performance of iterative versus QR decomposition methods for solving linear equations?\n",
    "Let's compare the performance of our iterative methods to the QR decomposition method for solving linear equations. We'll set up a benchmarking experiment using [the `@benchmark` macro exported from the `BenchmarkTools.jl` package](https://github.com/JuliaCI/BenchmarkTools.jl) to compare the time taken by each method to solve the same system of equations.\n",
    "\n",
    "Let's start with the QR decomposition method and then compare it to the iterative methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60b64bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m144.500 μs\u001b[22m\u001b[39m … \u001b[35m 3.511 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 94.36%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m150.708 μs              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m157.808 μs\u001b[22m\u001b[39m ± \u001b[32m63.769 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m3.19% ±  7.38%\n",
       "\n",
       "  \u001b[39m█\u001b[34m▄\u001b[39m\u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[34m█\u001b[39m\u001b[32m▄\u001b[39m\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m \u001b[39m▂\n",
       "  144 μs\u001b[90m          Histogram: frequency by time\u001b[39m          520 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m226.98 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m17\u001b[39m."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark begin\n",
    "    Q,R = qr($A);\n",
    "    x_qr = R \\ (transpose(Q) * $b);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3047bbbe",
   "metadata": {},
   "source": [
    "Next, let's look at the performance of our iterative methods. Use the same setup as above to get an apples-to-apples comparison of the time and memory taken by our iterative method versus the QR decomposition method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83a319b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 47 samples with 1 evaluation per sample.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m 96.796 ms\u001b[22m\u001b[39m … \u001b[35m275.874 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 4.06%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m103.830 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m4.80%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m109.587 ms\u001b[22m\u001b[39m ± \u001b[32m 26.945 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m3.96% ± 3.97%\n",
       "\n",
       "  \u001b[39m█\u001b[39m▄\u001b[34m▅\u001b[39m\u001b[39m▇\u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[32m▅\u001b[39m\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m \u001b[39m▁\n",
       "  96.8 ms\u001b[90m          Histogram: frequency by time\u001b[39m          276 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m104.86 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m232759\u001b[39m."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "    \n",
    "    # initialize -\n",
    "    number_of_rows = size(A, 1);\n",
    "    maximum_number_of_iterations = 25000; # maximum number of iterations for the iterative method\n",
    "    tolerance = 1e-12;\n",
    "    algorithm = GaussSeidelMethod(); # change this to Jacobi(), GaussSeidelMethod() or SuccessiveOverRelaxationMethod() to test other algorithms\n",
    "    ω = 0.6; # relaxation factor, only used for SOR\n",
    "    xₒ = 0.1*ones(number_of_rows); # initial solution guess xₒ\n",
    "\n",
    "     # call the solve method with the appropriate parameters -\n",
    "    @benchmark VLDataScienceMachineLearningPackage.solve($A, $b, $xₒ; algorithm = $algorithm, \n",
    "        maxiterations = $maximum_number_of_iterations, \n",
    "        ϵ = $tolerance, \n",
    "        ω = $ω # only used for SOR\n",
    "    );\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7019b5a5",
   "metadata": {},
   "source": [
    "__Wow!__ We are slow (and use a lot of memory) compared to the QR decomposition method for solving linear equations. Hmmm. Maybe we should consider doing some refactoring! But that is a task for another day. For now, let's just note that the QR decomposition method is significantly faster and more memory efficient than our iterative methods for solving linear equations in this case.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39b738",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this lab, we compared direct and iterative methods for solving linear systems using a manufactured solution approach for the 1D Poisson equation. We implemented QR decomposition and iterative solvers to understand their trade-offs in accuracy, convergence, and performance.\n",
    "\n",
    "> **Key takeaways:**\n",
    "> * **Direct methods outperform iterative methods for dense systems.** QR decomposition achieved machine precision accuracy and significantly faster execution times compared to Jacobi, Gauss-Seidel, and SOR methods on our tridiagonal test problem.\n",
    "> * **Manufactured solutions enable rigorous validation.** By constructing test problems with known exact solutions, we quantitatively measured solver accuracy and verified that our numerical methods converge to the correct answer.\n",
    "> * **Convergence theory predicts practical performance.** The diagonal dominance properties we checked theoretically guaranteed convergence of our iterative methods, demonstrating the connection between mathematical theory and computational results.\n",
    "\n",
    "In this lab, we explored square systems of linear equations. In practice, many real-world problems involve non-square systems. Thus, in future labs we will explore least squares solutions and regularization techniques for such systems.\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
